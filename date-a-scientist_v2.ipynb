{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff5719c4-3769-4b36-b061-15204251ad1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['english', 'spanish', 'french', 'chinese', 'german']\n",
      "body_type\n",
      "other       0.248802\n",
      "average     0.244561\n",
      "fit         0.212166\n",
      "athletic    0.197205\n",
      "unknown     0.097267\n",
      "Name: proportion, dtype: float64\n",
      "diet\n",
      "anything      0.465260\n",
      "unknown       0.406833\n",
      "vegetarian    0.094945\n",
      "other         0.032962\n",
      "Name: proportion, dtype: float64\n",
      "drinks\n",
      "socially    0.697263\n",
      "light       0.153957\n",
      "heavy       0.099187\n",
      "unknown     0.049593\n",
      "Name: proportion, dtype: float64\n",
      "drugs\n",
      "no         0.629502\n",
      "unknown    0.234809\n",
      "yes        0.135689\n",
      "Name: proportion, dtype: float64\n",
      "ethnicity\n",
      "white      0.547999\n",
      "other      0.255030\n",
      "asian      0.102359\n",
      "unknown    0.094612\n",
      "Name: proportion, dtype: float64\n",
      "income\n",
      "unknown    0.808256\n",
      "high       0.091990\n",
      "medium     0.050528\n",
      "low        0.049226\n",
      "Name: proportion, dtype: float64\n",
      "job\n",
      "other               0.209912\n",
      "business            0.173744\n",
      "stem                0.159484\n",
      "unknown             0.143821\n",
      "education_health    0.120043\n",
      "creative            0.111594\n",
      "student             0.081403\n",
      "Name: proportion, dtype: float64\n",
      "last_online\n",
      "active        0.509042\n",
      "not active    0.490958\n",
      "Name: proportion, dtype: float64\n",
      "orientation\n",
      "straight    0.860821\n",
      "queer       0.139179\n",
      "Name: proportion, dtype: float64\n",
      "religion\n",
      "unknown         0.337369\n",
      "other           0.171389\n",
      "agnosticism     0.147027\n",
      "atheism         0.116553\n",
      "christianity    0.096548\n",
      "catholicism     0.079450\n",
      "judaism         0.051664\n",
      "Name: proportion, dtype: float64\n",
      "sex\n",
      "m    0.597642\n",
      "f    0.402358\n",
      "Name: proportion, dtype: float64\n",
      "sign\n",
      "unknown        0.184280\n",
      "leo            0.072937\n",
      "gemini         0.071919\n",
      "libra          0.070216\n",
      "cancer         0.070199\n",
      "virgo          0.069130\n",
      "taurus         0.069047\n",
      "scorpio        0.068980\n",
      "aries          0.066575\n",
      "pisces         0.065841\n",
      "sagittarius    0.065757\n",
      "aquarius       0.065507\n",
      "capricorn      0.059612\n",
      "Name: proportion, dtype: float64\n",
      "smokes\n",
      "no         0.732480\n",
      "yes        0.175698\n",
      "unknown    0.091823\n",
      "Name: proportion, dtype: float64\n",
      "status\n",
      "available      0.960225\n",
      "unavailable    0.039608\n",
      "unknown        0.000167\n",
      "Name: proportion, dtype: float64\n",
      "education_level\n",
      "college    0.575450\n",
      "masters    0.182260\n",
      "unknown    0.138394\n",
      "other      0.103896\n",
      "Name: proportion, dtype: float64\n",
      "city\n",
      "san francisco    0.518176\n",
      "other            0.481824\n",
      "Name: proportion, dtype: float64\n",
      "likes_dogs\n",
      "yes    0.624493\n",
      "no     0.375507\n",
      "Name: proportion, dtype: float64\n",
      "likes_cats\n",
      "no     0.522584\n",
      "yes    0.477416\n",
      "Name: proportion, dtype: float64\n",
      "religion_importance\n",
      "unknown    0.533972\n",
      "no         0.353750\n",
      "yes        0.112278\n",
      "Name: proportion, dtype: float64\n",
      "sign_importance\n",
      "unknown          0.386595\n",
      "fun              0.322624\n",
      "not important    0.279560\n",
      "important        0.011221\n",
      "Name: proportion, dtype: float64\n",
      "english_level\n",
      "okay        0.519245\n",
      "fluently    0.469301\n",
      "poorly      0.010620\n",
      "none        0.000835\n",
      "Name: proportion, dtype: float64\n",
      "spanish_level\n",
      "none        0.727821\n",
      "okay        0.116453\n",
      "poorly      0.104747\n",
      "fluently    0.050979\n",
      "Name: proportion, dtype: float64\n",
      "french_level\n",
      "none        0.869020\n",
      "poorly      0.060430\n",
      "okay        0.052482\n",
      "fluently    0.018067\n",
      "Name: proportion, dtype: float64\n",
      "chinese_level\n",
      "none        0.938952\n",
      "okay        0.026383\n",
      "fluently    0.019169\n",
      "poorly      0.015496\n",
      "Name: proportion, dtype: float64\n",
      "german_level\n",
      "none        0.948587\n",
      "poorly      0.025047\n",
      "okay        0.017967\n",
      "fluently    0.008399\n",
      "Name: proportion, dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 59887 entries, 0 to 59945\n",
      "Data columns (total 28 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   age                  59887 non-null  int64  \n",
      " 1   body_type            59887 non-null  object \n",
      " 2   diet                 59887 non-null  object \n",
      " 3   drinks               59887 non-null  object \n",
      " 4   drugs                59887 non-null  object \n",
      " 5   ethnicity            59887 non-null  object \n",
      " 6   height               59887 non-null  float64\n",
      " 7   income               59887 non-null  object \n",
      " 8   job                  59887 non-null  object \n",
      " 9   last_online          59887 non-null  object \n",
      " 10  orientation          59887 non-null  object \n",
      " 11  religion             59887 non-null  object \n",
      " 12  sex                  59887 non-null  object \n",
      " 13  sign                 59887 non-null  object \n",
      " 14  smokes               59887 non-null  object \n",
      " 15  status               59887 non-null  object \n",
      " 16  education_level      59887 non-null  object \n",
      " 17  city                 59887 non-null  object \n",
      " 18  likes_dogs           59887 non-null  object \n",
      " 19  likes_cats           59887 non-null  object \n",
      " 20  religion_importance  59887 non-null  object \n",
      " 21  sign_importance      59887 non-null  object \n",
      " 22  language_count       59887 non-null  int64  \n",
      " 23  english_level        59887 non-null  object \n",
      " 24  spanish_level        59887 non-null  object \n",
      " 25  french_level         59887 non-null  object \n",
      " 26  chinese_level        59887 non-null  object \n",
      " 27  german_level         59887 non-null  object \n",
      "dtypes: float64(1), int64(2), object(25)\n",
      "memory usage: 15.3+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PedroMarques\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    would love think kind intellectual either dumb...\n",
      "1    chef means 1 workaholic 2 love cook regardless...\n",
      "2    im ashamed much writing public text online dat...\n",
      "3    work library go school reading things written ...\n",
      "4    hey hows going currently vague profile know co...\n",
      "Name: essays, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "df = pd.read_csv('profiles.csv')\n",
    "\n",
    "#print(df.describe())\n",
    "#print(df.info())\n",
    "#df.head()\n",
    "\n",
    "#EDA\n",
    "\n",
    "#age column\n",
    "#plt.hist(df['age'])\n",
    "#plt.show() #based on the histogram, let's consider that everything above 80 years in wrong and drop those rows\n",
    "df = df[df['age'] < 80]\n",
    "\n",
    "#body type\n",
    "# 3 groups with ~20% of the answers each. All the others have less than 10%. Will keep those 3 and aggregate the others\n",
    "\n",
    "map_body_type = {\n",
    "    'average': 'average',\n",
    "    'fit': 'fit', \n",
    "    'athletic': 'athletic',  \n",
    "    'thin': 'other', \n",
    "    'curvy': 'other',\n",
    "    'a little extra': 'other',  \n",
    "    'skinny': 'other', \n",
    "    'full figured': 'other',  \n",
    "    'overweight': 'other',  \n",
    "    'jacked': 'other', \n",
    "    'used up': 'unknown',  \n",
    "    'rather not say': 'unknown'\n",
    "}\n",
    "\n",
    "df['body_type'] = df['body_type'].map(map_body_type).fillna('unknown')\n",
    "\n",
    "#diet\n",
    "map_diet = {\n",
    "    # Group 1: Anything\n",
    "    'mostly anything': 'anything',\n",
    "    'anything': 'anything',\n",
    "    'strictly anything': 'anything',\n",
    "    \n",
    "    # Group 2: Vegetarian/Vegan\n",
    "    'vegetarian': 'vegetarian',\n",
    "    'mostly vegetarian': 'vegetarian',\n",
    "    'strictly vegetarian': 'vegetarian',\n",
    "    'vegan': 'vegetarian',\n",
    "    'mostly vegan': 'vegetarian',\n",
    "    'strictly vegan': 'vegetarian',\n",
    "    \n",
    "    # Group 3: Other (includes religious-based + remaining 'other')\n",
    "    'other': 'other',\n",
    "    'mostly other': 'other',\n",
    "    'strictly other': 'other',\n",
    "    'kosher': 'other',\n",
    "    'mostly kosher': 'other',\n",
    "    'strictly kosher': 'other',\n",
    "    'halal': 'other',\n",
    "    'mostly halal': 'other',\n",
    "    'strictly halal': 'other'\n",
    "}\n",
    "\n",
    "df['diet'] = df['diet'].map(map_diet).fillna('unknown')\n",
    "# ~40% of unknowns, ~50% of anything. Might consider dropping this one\n",
    "\n",
    "#drinks\n",
    "\n",
    "map_drinks = {\n",
    "    'socially': 'socially',\n",
    "    'rarely': 'light',\n",
    "    'not at all': 'light',\n",
    "    'often': 'heavy',\n",
    "    'very often': 'heavy',\n",
    "    'desperately': 'heavy'\n",
    "}\n",
    "df['drinks'] = df['drinks'].map(map_drinks).fillna('unknown')\n",
    "\n",
    "#socially category dominates with 70%\n",
    "\n",
    "#drugs\n",
    "map_drugs = {\n",
    "    'never': 'no',\n",
    "    'sometimes': 'yes',\n",
    "    'often': 'yes',\n",
    "    'unknown': 'unknown'\n",
    "}\n",
    "\n",
    "df['drugs'] = df['drugs'].map(map_drugs).fillna('unknown')\n",
    "\n",
    "#education\n",
    "\n",
    "#will divide into education level and is student\n",
    "\n",
    "education_level_conditions = [\n",
    "    df['education'].str.contains('college', case = False, na=False),\n",
    "    df['education'].str.contains('masters', case = False, na=False),\n",
    "    df['education'].str.contains('high school|ph.d|law school|med school', case = False, na=False) #these represent a low percentage <5%, so will group them\n",
    "]\n",
    "\n",
    "education_level_choices = ['college', 'masters', 'other']\n",
    "\n",
    "df['education_level'] = np.select(education_level_conditions, education_level_choices, default = 'unknown')\n",
    "\n",
    "#print(df['education_level'].value_counts(normalize = True))\n",
    "\n",
    "is_student_conditions = [\n",
    "    df['education'].str.contains('working on', case = False, na=False),\n",
    "    df['education'].str.contains('graduated from|dropped out', case = False, na=False),\n",
    "    df['education'].str.contains('space camp', case = False, na=False) #will consider all these unknown    \n",
    "]\n",
    "\n",
    "is_student_choices = ['yes', 'no', 'unknown']\n",
    "\n",
    "df['is_student'] = np.select(is_student_conditions, is_student_choices, default = 'unknown')\n",
    "\n",
    "df = df.drop('education', axis = 1)\n",
    "\n",
    "ethnicity_conditions = [\n",
    "    df['ethnicity'] == 'white',\n",
    "    df['ethnicity'] == 'asian',\n",
    "    df['ethnicity'].isna()\n",
    "]\n",
    "\n",
    "ethnicity_choices = ['white', 'asian', 'unknown']\n",
    "\n",
    "df['ethnicity'] = np.select(ethnicity_conditions, ethnicity_choices, default = 'other')\n",
    "\n",
    "#height\n",
    "#plt.hist(df['height'])\n",
    "#plt.show()\n",
    "df = df[(df['height'] >= 50) & (df['height'] <= 90)] #this is a reasonable interval for height\n",
    "\n",
    "#income\n",
    "# 80% of unknowns, not a great column to predict other columns, but might be fun to remove the unknowns and predict this one\n",
    "\n",
    "#plt.hist(df_income['income'])\n",
    "#plt.show()\n",
    "\n",
    "def classify_income(i):\n",
    "    if i <= 0:\n",
    "        return 'unknown'\n",
    "    elif i <= 20000:\n",
    "        return 'low'\n",
    "    elif i <= 50000:\n",
    "        return 'medium'\n",
    "    else: \n",
    "        return 'high'\n",
    "\n",
    "df['income'] = df['income'].apply(classify_income)\n",
    "\n",
    "#print(df['income'].value_counts(normalize = True))\n",
    "\n",
    "#job\n",
    "\n",
    "job_map = {\n",
    "    # STEM/Technical\n",
    "    'science / tech / engineering': 'stem',\n",
    "    'computer / hardware / software': 'stem',\n",
    "\n",
    "    # Creative/Artistic\n",
    "    'artistic / musical / writer': 'creative',\n",
    "    'entertainment / media': 'creative',\n",
    "\n",
    "    # Business/Professional\n",
    "    'sales / marketing / biz dev': 'business',\n",
    "    'executive / management': 'business',\n",
    "    'banking / financial / real estate': 'business',\n",
    "    'law / legal services': 'business',\n",
    "\n",
    "    # Education/Health\n",
    "    'education / academia': 'education_health',\n",
    "    'medicine / health': 'education_health',\n",
    "\n",
    "    # Manual Labor / Skilled Trades - Not enough percentage to create its own category\n",
    "    'construction / craftsmanship': 'other',\n",
    "    'transportation': 'other',\n",
    "    'clerical / administrative': 'other',\n",
    "\n",
    "    # Public/Government - Not enough percentage to create its own category\n",
    "    'political / government': 'other',\n",
    "    'military': 'other',\n",
    "\n",
    "    # Other groups\n",
    "    'student': 'student',\n",
    "    'unemployed': 'other',\n",
    "    'retired': 'other',\n",
    "    'rather not say': 'unknown',\n",
    "    'hospitality / travel': 'other',\n",
    "    'other': 'other'\n",
    "}\n",
    "\n",
    "df['job'] = df['job'].map(job_map).fillna('unknown')\n",
    "\n",
    "#last online\n",
    "\n",
    "df['last_online'] = pd.to_datetime(df['last_online'], format='%Y-%m-%d-%H-%M')\n",
    "most_recent_date = df['last_online'].max()\n",
    "df['last_online'] = (most_recent_date - df['last_online']).dt.days\n",
    "\n",
    "#binary distribution into active or not active. Could also make a semi active category (<=1, <=3, >3)\n",
    "def categorize_last_online(v):\n",
    "    if v <= 3:\n",
    "        return 'active'\n",
    "    else:\n",
    "        return 'not active'\n",
    "\n",
    "df['last_online'] = df['last_online'].apply(categorize_last_online)\n",
    "\n",
    "#location\n",
    "\n",
    "def get_city(location):\n",
    "    city = location.split(',')[0]\n",
    "    if city == 'san francisco': #50% of the states are san francisco, so will do this and 'other'\n",
    "        return city\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "df['city'] = df['location'].apply(get_city)\n",
    "\n",
    "#offspring\n",
    "df['offspring'] = df['offspring'].fillna('unknown')\n",
    "\n",
    "#will drop the column because there are 60% of unknowns and spliting between has and wants kids made it worse.\n",
    "\n",
    "df.drop('offspring', axis = 1)\n",
    "\n",
    "#orientation\n",
    "\n",
    "map_orientation = {\n",
    "    'straight': 'straight',\n",
    "    'gay': 'queer',\n",
    "    'bisexual': 'queer'\n",
    "}\n",
    "\n",
    "df['orientation'] = df['orientation'].map(map_orientation).fillna('unknown')\n",
    "\n",
    "#pets\n",
    "df['pets'] = df['pets'].fillna('unknown')\n",
    "\n",
    "def likes_dogs(pets):\n",
    "    if isinstance(pets, str):\n",
    "        pets = pets.lower().strip()\n",
    "        if 'likes dogs' in pets or 'has dogs' in pets:\n",
    "            return 'yes'\n",
    "    return 'no'\n",
    "\n",
    "def likes_cats(pets):\n",
    "    if isinstance(pets, str):\n",
    "        pets = pets.lower().strip()\n",
    "        if 'likes cats' in pets or 'has cats' in pets:\n",
    "            return 'yes'\n",
    "    return 'no'\n",
    "        \n",
    "\n",
    "df['likes_dogs'] = df['pets'].apply(likes_dogs)\n",
    "df['likes_cats'] = df['pets'].apply(likes_cats)\n",
    "\n",
    "#religion\n",
    "df['religion'] = df['religion'].fillna('unknown')\n",
    "\n",
    "def get_religion(r):\n",
    "    try:\n",
    "        religion = r.split(' ')[0]\n",
    "    except:\n",
    "        print(r)\n",
    "        return\n",
    "    if religion == 'buddhism' or religion == 'hinduism' or religion == 'islam':\n",
    "        return 'other'\n",
    "    else:\n",
    "        return religion\n",
    "\n",
    "religion_importance_conditions = [\n",
    "    df['religion'].str.contains('not too serious|laughing', case=False, na=False),\n",
    "    df['religion'].str.contains('somewhat serious|very serious', case=False, na=False),\n",
    "    df['religion'].str.contains('unknown', case=False, na=False)\n",
    "]\n",
    "\n",
    "religion_importance_options = ['no', 'yes', 'unknown']\n",
    "\n",
    "df['religion_importance'] = np.select(religion_importance_conditions, religion_importance_options, default = 'unknown')\n",
    "df['religion'] = df['religion'].apply(get_religion)   \n",
    "\n",
    "#sex - good to go\n",
    "#print(df['sex'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "#sign\n",
    "df['sign'] = df['sign'].fillna('unknown')\n",
    "df['sign'] = df['sign'].str.replace('&rsquo;', '\\'', regex=False)\n",
    "\n",
    "def get_sign(sign):\n",
    "    if isinstance(sign, str):\n",
    "        return sign.split(' ')[0]\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "sign_importance_conditions = [\n",
    "    df['sign'].str.contains('fun', regex = False),\n",
    "    df['sign'].str.contains('but it doesn\\'t matter', regex = False),\n",
    "    df['sign'].str.contains('and it matters a lot', regex = False)\n",
    "]\n",
    "\n",
    "sign_importance_options = ['fun', 'not important', 'important']\n",
    "\n",
    "df['sign_importance'] = np.select(sign_importance_conditions, sign_importance_options, default = 'unknown')\n",
    "df['sign'] = df['sign'].apply(get_sign)\n",
    "\n",
    "#print(df['sign'].value_counts(normalize=True, dropna=False)) # 12 categories + unknown -> not sure if it will have predictive power. Might drop it\n",
    "#print(df['sign_importance'].value_counts(normalize=True, dropna=False))  # only 1% considers important, might drop it\n",
    "\n",
    "df.drop(['sign', 'sign_importance'], axis = 1)\n",
    "\n",
    "#smokes\n",
    "df['smokes'] = df['smokes'].fillna('unknown')\n",
    "\n",
    "smokes_map = {\n",
    "    'no': 'no',\n",
    "    'unknown': 'unknown',\n",
    "    'sometimes': 'yes',\n",
    "    'when drinking': 'yes',\n",
    "    'trying to quit': 'yes',\n",
    "    'yes': 'yes'\n",
    "}\n",
    "\n",
    "df['smokes'] = df['smokes'].map(smokes_map)\n",
    "\n",
    "#print(df['smokes'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "#speaks - has a list of comma separated values with proficiency between brackets\n",
    "# will get the top 5 languages and save the proficiency level for each + add a column for languages count\n",
    "df['speaks'] = df['speaks'].fillna('unknown')\n",
    "\n",
    "def get_languages_count(l):\n",
    "    count_commas = l.count(',')\n",
    "    return (count_commas + 1)\n",
    "\n",
    "df['language_count'] = df['speaks'].apply(get_languages_count)\n",
    "#print(df['language_count'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "#TO DO -> get the 5 top languages and create columns with the level\n",
    "\n",
    "language_counts = {}\n",
    "def get_top_languages(lang):\n",
    "    languages = lang.split(',')\n",
    "    for language in languages:\n",
    "        l = language.strip().split(' ')[0]\n",
    "        if l in language_counts:\n",
    "            language_counts[l] += 1\n",
    "        else:\n",
    "            language_counts[l] = 1\n",
    "\n",
    "df['speaks'].apply(get_top_languages)\n",
    "top_5_languages = sorted(language_counts, key=language_counts.get, reverse=True)[:5]\n",
    "print(top_5_languages)\n",
    "\n",
    "for lang in top_5_languages:\n",
    "    df[f'{lang}_level'] = 'none'\n",
    "\n",
    "for idx, row in df['speaks'].items():\n",
    "    languages = row.split(',')\n",
    "    for language in languages:\n",
    "        record = language.strip().split(' ')\n",
    "        l = record[0].strip().lower()\n",
    "        if len(record) == 1:\n",
    "            level = 'okay'\n",
    "        elif len(record) >= 2:\n",
    "            level = record[1].strip('()').lower()\n",
    "        if l in top_5_languages:\n",
    "            df.at[idx, f'{l}_level'] = level        \n",
    "\n",
    "#status\n",
    "df['status'] = df['status'].fillna('unknown')\n",
    "#print(df['status'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "status_map = {\n",
    "    'single': 'available',\n",
    "    'seeing someone': 'unavailable',\n",
    "    'available': 'available',\n",
    "    'married': 'unavailable',\n",
    "    'unknown': 'unknown'\n",
    "}\n",
    "\n",
    "df['status'] = df['status'].map(status_map)\n",
    "\n",
    "#print(df['status'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "df = df.drop(['offspring', 'location', 'pets', 'speaks', 'is_student'], axis=1)\n",
    "not_essay_cols = [col for col in df.columns if 'essay' not in col]\n",
    "essay_cols = [col for col in df.columns if 'essay' in col]\n",
    "for col in not_essay_cols:\n",
    "    if df[col].dtype in ['object', 'category']:\n",
    "        print(df[col].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "print(df[not_essay_cols].info())\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    try:\n",
    "        # Ensure it's a string\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # Remove stop words\n",
    "        words = text.split()\n",
    "        filtered = [word for word in words if word not in stop_words]\n",
    "\n",
    "        return ' '.join(filtered)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}\\nError: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    \n",
    "\n",
    "df['essays'] = df[essay_cols].fillna('').agg(' '.join, axis=1)\n",
    "df['essays'] = df['essays'].apply(preprocess)\n",
    "print(df['essays'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab4aab8b-837a-4f11-b064-b8d8301602cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cell...\n",
      "Starting downsample...\n",
      "Creating cols...\n",
      "Preparing tabular data...\n",
      "Starting split...\n",
      "Starting stacking cell...\n",
      "Classification report for stacking:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.75      0.77      0.76      1626\n",
      "         yes       0.76      0.74      0.75      1625\n",
      "\n",
      "    accuracy                           0.76      3251\n",
      "   macro avg       0.76      0.76      0.76      3251\n",
      "weighted avg       0.76      0.76      0.76      3251\n",
      "\n",
      "Starting text stacking cell...\n",
      "Classification report for stacking:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.68      0.74      0.71      1626\n",
      "         yes       0.72      0.66      0.69      1625\n",
      "\n",
      "    accuracy                           0.70      3251\n",
      "   macro avg       0.70      0.70      0.70      3251\n",
      "weighted avg       0.70      0.70      0.70      3251\n",
      "\n",
      "Starting meta features cell...\n",
      "üîç Classification Report for Stacking + Naive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.75      0.78      0.76      1626\n",
      "         yes       0.77      0.74      0.76      1625\n",
      "\n",
      "    accuracy                           0.76      3251\n",
      "   macro avg       0.76      0.76      0.76      3251\n",
      "weighted avg       0.76      0.76      0.76      3251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def downsample(df, col):\n",
    "    min_count = df[col].value_counts().min()\n",
    "    balanced_df = pd.concat([\n",
    "        df[df[col] == category].sample(n=min_count, random_state=1)\n",
    "        for category in df[col].unique()\n",
    "    ])\n",
    "    return balanced_df\n",
    "\n",
    "# ----------- Recreate a consistent train/test split ----------- #\n",
    "\n",
    "print(\"Starting cell...\")\n",
    "\n",
    "# Create full dataset again for consistent split\n",
    "full_df = df.copy()\n",
    "col_to_predict = 'drugs'\n",
    "full_df = full_df[full_df[col_to_predict] != 'unknown']\n",
    "full_df = full_df[full_df[col_to_predict].notna()]\n",
    "\n",
    "print(\"Starting downsample...\")\n",
    "\n",
    "full_df = downsample(full_df, col_to_predict)\n",
    "\n",
    "# Label encode target\n",
    "#le = LabelEncoder()\n",
    "#full_df[col_to_predict] = le.fit_transform(full_df[col_to_predict])\n",
    "#y_full = full_df[col_to_predict].values\n",
    "y_full = full_df[col_to_predict].values\n",
    "\n",
    "print(\"Creating cols...\")\n",
    "#create vars with categorical columns and numerical columns\n",
    "cat_cols = full_df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "cat_cols = [col for col in cat_cols if col != col_to_predict and 'essay' not in col]\n",
    "num_cols = full_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "num_cols = [col for col in num_cols if col != col_to_predict]\n",
    "\n",
    "print(\"Preparing tabular data...\")\n",
    "# Prepare tabular data\n",
    "df_dummies = pd.get_dummies(full_df[cat_cols], drop_first=True)\n",
    "X_tab = pd.concat([full_df[num_cols], df_dummies], axis=1)\n",
    "X_tab = X_tab.reset_index(drop=True)\n",
    "\n",
    "# Standard scale numeric columns\n",
    "scaler = StandardScaler()\n",
    "if len(num_cols) > 0: # check if there are numerical columns\n",
    "    X_tab[num_cols] = scaler.fit_transform(X_tab[num_cols])\n",
    "\n",
    "# Prepare text data\n",
    "X_text = full_df['essays'].fillna(\"\").reset_index(drop=True)\n",
    "\n",
    "print(\"Starting split...\")\n",
    "\n",
    "# ----------- Consistent split using index ----------- #\n",
    "\n",
    "idx_train, idx_test = train_test_split(np.arange(len(full_df)), test_size=0.2, stratify=y_full, random_state=42)\n",
    "\n",
    "X_tab_train, X_tab_test = X_tab.iloc[idx_train], X_tab.iloc[idx_test]\n",
    "X_text_train, X_text_test = X_text.iloc[idx_train], X_text.iloc[idx_test]\n",
    "y_train, y_test = y_full[idx_train], y_full[idx_test]\n",
    "\n",
    "# ----------- Train Stacking Model (Tabular) ----------- #\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"Starting stacking cell...\")\n",
    "\n",
    "base_models = [\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=100)),\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "]\n",
    "\n",
    "meta_tabular = LogisticRegression(max_iter=1000)\n",
    "stack = StackingClassifier(estimators=base_models, final_estimator=meta_tabular, cv=5, n_jobs=-1, passthrough=True)\n",
    "\n",
    "stack.fit(X_tab_train, y_train)\n",
    "y_pred = stack.predict(X_tab_test)\n",
    "stack_score = classification_report(y_test, y_pred)\n",
    "print('Classification report for stacking:')\n",
    "print(stack_score)\n",
    "\n",
    "\n",
    "# ----------- Train Text Model (Naive Bayes) ----------- #\n",
    "print(\"Starting text stacking cell...\")\n",
    "\n",
    "text_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english', max_features=5000)),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "text_pipeline.fit(X_text_train, y_train)\n",
    "y_pred = text_pipeline.predict(X_text_test)\n",
    "text_pipeline_score = classification_report(y_test, y_pred)\n",
    "print('Classification report for stacking:')\n",
    "print(text_pipeline_score)\n",
    "\n",
    "# ----------- Generate meta-features ----------- #\n",
    "print(\"Starting meta features cell...\")\n",
    "# Probabilities from stack model\n",
    "pred_tab_train = stack.predict_proba(X_tab_train)  # shape [n_samples, n_classes]\n",
    "pred_tab_test = stack.predict_proba(X_tab_test)\n",
    "\n",
    "# Probabilities from text model\n",
    "pred_text_train = text_pipeline.predict_proba(X_text_train)\n",
    "pred_text_test = text_pipeline.predict_proba(X_text_test)\n",
    "\n",
    "# Stack horizontally\n",
    "X_meta_train = np.hstack((pred_tab_train, pred_text_train))\n",
    "X_meta_test = np.hstack((pred_tab_test, pred_text_test))\n",
    "\n",
    "# ----------- Train Final Meta-Model ----------- #\n",
    "\n",
    "final_meta_model = LogisticRegression(max_iter=1000)\n",
    "final_meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# ----------- Evaluate ----------- #\n",
    "\n",
    "final_preds = final_meta_model.predict(X_meta_test)\n",
    "print('üîç Classification Report for Stacking + Naive Bayes:')\n",
    "print(classification_report(y_test, final_preds, target_names=sorted(set(y_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b24db-199a-4779-8f74-f835a0ff2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, SelectKBest, chi2\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) #make sure warning do not appear for the grid search\n",
    "\n",
    "\n",
    "def downsample(df, col):\n",
    "    min_count = df[col].value_counts().min()\n",
    "    balanced_df = pd.concat([\n",
    "        df[df[col] == category].sample(n=min_count, random_state=1)\n",
    "        for category in df[col].unique()\n",
    "    ])\n",
    "    return balanced_df\n",
    "\n",
    "data = df[not_essay_cols].copy()\n",
    "col_to_predict = 'job'\n",
    "print(data[col_to_predict].value_counts())\n",
    "\n",
    "#remove rows where the target variable is unknown\n",
    "data = data[data[col_to_predict] != 'unknown']\n",
    "data = data[data[col_to_predict].notna()]\n",
    "\n",
    "data = downsample(data, col_to_predict)\n",
    "\n",
    "#create vars with categorical columns and numerical columns\n",
    "cat_cols = data.select_dtypes(include=['object','category']).columns.tolist()\n",
    "cat_cols = [col for col in cat_cols if col != col_to_predict]\n",
    "num_cols = data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "num_cols = [col for col in num_cols if col != col_to_predict]\n",
    "\n",
    "df_dummies = pd.get_dummies(data[cat_cols], drop_first = True)\n",
    "\n",
    "x = pd.concat([data[num_cols], df_dummies], axis = 1)\n",
    "y = data[col_to_predict]\n",
    "\n",
    "print(data[col_to_predict].value_counts()) #check if the classes are balanced\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(le.classes_)\n",
    "\n",
    "# ------------ Chi2 test ------------ #\n",
    "\n",
    "# Chi2 requires all values to be non-negative\n",
    "X_chi = x.copy()\n",
    "X_chi[X_chi < 0] = 0  # Only necessary if you have negative values\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=10)  # Keep top 10 features\n",
    "X_chi_selected = selector.fit_transform(X_chi, y)\n",
    "selected_features = X_chi.columns[selector.get_support()]\n",
    "print(f\"Selected features from Chi2: {list(selected_features)}\")\n",
    "\n",
    "x = x[selected_features]\n",
    "\n",
    "cat_cols = x.select_dtypes(include=['object','category']).columns.tolist()\n",
    "num_cols = x.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "\n",
    "# ------------ End of Chi2 test ------------ #\n",
    "\n",
    "# Split once for both\n",
    "train_idx, test_idx = train_test_split(np.arange(len(data)), test_size=0.2, stratify=data[col_to_predict], random_state=42)\n",
    "\n",
    "# Apply same split\n",
    "x_tab_train = x.iloc[train_idx]\n",
    "x_tab_test = x.iloc[test_idx]\n",
    "x_text_train = x_essay[train_idx]\n",
    "x_text_test = x_essay[test_idx]\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "# the purpose of this split is to have x tabular values (nums and cats) to use in LR, DT, etc.. and also the essays to use in Naive Bayes\n",
    "\n",
    "scaler = StandardScaler()\n",
    "if len(num_cols) > 0:\n",
    "    x_train[num_cols] = scaler.fit_transform(x_train[num_cols])\n",
    "    x_test[num_cols] = scaler.transform(x_test[num_cols]) #never use fit transform on test data, because it will learn from train data\n",
    "\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "\n",
    "\n",
    "# --- XGB Classifier --- #\n",
    "\n",
    "model = XGBClassifier(class_weight='balanced')\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1_macro', verbose=1, n_jobs=-1)    \n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print('Classification report for XGB Classifier:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "#print(\"Best Parameters:\", gs.best_params_)\n",
    "\n",
    "# --- Logistic Regression --- #\n",
    "model = LogisticRegression(max_iter = 1000, class_weight='balanced')\n",
    "\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': [1,3,5,7],  # Inverse of regularization strength\n",
    "    'solver': ['liblinear', 'saga'],  # Solvers that support l1 and elasticnet\n",
    "    'l1_ratio': [0, 0.5, 1]  # Only used with elasticnet\n",
    "}\n",
    "gs = GridSearchCV(model, param_grid, cv=5, scoring='f1_weighted', verbose=1, n_jobs=-1)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "y_pred_model = model.predict(x_test)\n",
    "model_score = classification_report(y_test, y_pred_model)\n",
    "print('Classification report for LR:')\n",
    "print(model_score)\n",
    "#print(\"Best Parameters:\", gs.best_params_)\n",
    "\n",
    "# --- K-Nearest Neighbors ---\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict(x_test)\n",
    "model_score = classification_report(y_test, y_pred)\n",
    "print('Classification report for KNN:')\n",
    "print(model_score)\n",
    "\n",
    "# --- Decision tree --- #\n",
    "dtree = DecisionTreeClassifier(max_depth=3, random_state=42, class_weight='balanced')\n",
    "dtree.fit(x_train, y_train)\n",
    "y_pred = dtree.predict(x_test)\n",
    "model_score = classification_report(y_test, y_pred)\n",
    "print('Classification report for decision tree:')\n",
    "print(model_score)\n",
    "\n",
    "# --- Stacking --- #\n",
    "\n",
    "base_models = [\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=100)),\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "]\n",
    "\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "stack = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1, passthrough=True)\n",
    "\n",
    "stack.fit(x_tab_train, y_train)\n",
    "y_pred = stack.predict(x_tab_test)\n",
    "model_score = classification_report(y_test, y_pred)\n",
    "print('Classification report for stacking:')\n",
    "print(model_score)\n",
    "\n",
    "# --- Naive Bayes --- #\n",
    "data = df.copy()\n",
    "\n",
    "data = data[data[col_to_predict] != 'unknown']\n",
    "data = data[data[col_to_predict].notna()]\n",
    "\n",
    "data = downsample(data, col_to_predict)\n",
    "\n",
    "y = data[col_to_predict]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "x_essay = vectorizer.fit_transform(data['essays'])\n",
    "\n",
    "x_essay_train, x_essay_test, y_train, y_test = train_test_split(x_essay, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_essay_train, y_train)\n",
    "y_pred = model.predict(x_essay_test)\n",
    "print('Classification report for Naive Bayes:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --- Stack Stacking + Naive Bayes --- #\n",
    "\n",
    "# Probabilities from stacked models on validation\n",
    "pred_stack_auto_val = stack.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Probabilities from text model\n",
    "pred_text_val = model.predict_proba(x_essay_test)[:, 1]\n",
    "\n",
    "# Combine both as new meta-features\n",
    "X_meta_val = np.hstack((pred_stack_auto_val, pred_text_val))\n",
    "print(X_meta_val)\n",
    "\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(X_meta_val, y_train)\n",
    "\n",
    "pred_stack_auto_test = stack.predict_proba(x_test)\n",
    "pred_text_test = model.predict_proba(x_essay_test)\n",
    "\n",
    "X_meta_test = np.column_stack((pred_stack_auto_test, pred_text_test))\n",
    "final_preds = meta_model.predict(X_meta_test)\n",
    "print('Classification report for stacking + Naive Bayes:')\n",
    "print(classification_report(y_test, final_preds))\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
