{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff5719c4-3769-4b36-b061-15204251ad1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59946 entries, 0 to 59945\n",
      "Data columns (total 31 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   age          59946 non-null  int64  \n",
      " 1   body_type    54650 non-null  object \n",
      " 2   diet         35551 non-null  object \n",
      " 3   drinks       56961 non-null  object \n",
      " 4   drugs        45866 non-null  object \n",
      " 5   education    53318 non-null  object \n",
      " 6   essay0       54458 non-null  object \n",
      " 7   essay1       52374 non-null  object \n",
      " 8   essay2       50308 non-null  object \n",
      " 9   essay3       48470 non-null  object \n",
      " 10  essay4       49409 non-null  object \n",
      " 11  essay5       49096 non-null  object \n",
      " 12  essay6       46175 non-null  object \n",
      " 13  essay7       47495 non-null  object \n",
      " 14  essay8       40721 non-null  object \n",
      " 15  essay9       47343 non-null  object \n",
      " 16  ethnicity    54266 non-null  object \n",
      " 17  height       59943 non-null  float64\n",
      " 18  income       59946 non-null  int64  \n",
      " 19  job          51748 non-null  object \n",
      " 20  last_online  59946 non-null  object \n",
      " 21  location     59946 non-null  object \n",
      " 22  offspring    24385 non-null  object \n",
      " 23  orientation  59946 non-null  object \n",
      " 24  pets         40025 non-null  object \n",
      " 25  religion     39720 non-null  object \n",
      " 26  sex          59946 non-null  object \n",
      " 27  sign         48890 non-null  object \n",
      " 28  smokes       54434 non-null  object \n",
      " 29  speaks       59896 non-null  object \n",
      " 30  status       59946 non-null  object \n",
      "dtypes: float64(1), int64(2), object(28)\n",
      "memory usage: 14.2+ MB\n",
      "None\n",
      "['english', 'spanish', 'french', 'chinese', 'german']\n",
      "body_type\n",
      "other       0.248802\n",
      "average     0.244561\n",
      "fit         0.212166\n",
      "athletic    0.197205\n",
      "unknown     0.097267\n",
      "Name: proportion, dtype: float64\n",
      "diet\n",
      "anything      0.465260\n",
      "unknown       0.406833\n",
      "vegetarian    0.094945\n",
      "other         0.032962\n",
      "Name: proportion, dtype: float64\n",
      "drinks\n",
      "socially    0.697263\n",
      "light       0.153957\n",
      "heavy       0.099187\n",
      "unknown     0.049593\n",
      "Name: proportion, dtype: float64\n",
      "drugs\n",
      "no         0.629502\n",
      "unknown    0.234809\n",
      "yes        0.135689\n",
      "Name: proportion, dtype: float64\n",
      "ethnicity\n",
      "white      0.547999\n",
      "other      0.255030\n",
      "asian      0.102359\n",
      "unknown    0.094612\n",
      "Name: proportion, dtype: float64\n",
      "income\n",
      "unknown    0.808256\n",
      "high       0.091990\n",
      "medium     0.050528\n",
      "low        0.049226\n",
      "Name: proportion, dtype: float64\n",
      "job\n",
      "other               0.209912\n",
      "business            0.173744\n",
      "stem                0.159484\n",
      "unknown             0.143821\n",
      "education_health    0.120043\n",
      "creative            0.111594\n",
      "student             0.081403\n",
      "Name: proportion, dtype: float64\n",
      "last_online\n",
      "active        0.509042\n",
      "not active    0.490958\n",
      "Name: proportion, dtype: float64\n",
      "orientation\n",
      "straight    0.860821\n",
      "queer       0.139179\n",
      "Name: proportion, dtype: float64\n",
      "religion\n",
      "unknown         0.337369\n",
      "other           0.171389\n",
      "agnosticism     0.147027\n",
      "atheism         0.116553\n",
      "christianity    0.096548\n",
      "catholicism     0.079450\n",
      "judaism         0.051664\n",
      "Name: proportion, dtype: float64\n",
      "sex\n",
      "m    0.597642\n",
      "f    0.402358\n",
      "Name: proportion, dtype: float64\n",
      "sign\n",
      "unknown        0.184280\n",
      "leo            0.072937\n",
      "gemini         0.071919\n",
      "libra          0.070216\n",
      "cancer         0.070199\n",
      "virgo          0.069130\n",
      "taurus         0.069047\n",
      "scorpio        0.068980\n",
      "aries          0.066575\n",
      "pisces         0.065841\n",
      "sagittarius    0.065757\n",
      "aquarius       0.065507\n",
      "capricorn      0.059612\n",
      "Name: proportion, dtype: float64\n",
      "smokes\n",
      "no         0.732480\n",
      "yes        0.175698\n",
      "unknown    0.091823\n",
      "Name: proportion, dtype: float64\n",
      "status\n",
      "available      0.960225\n",
      "unavailable    0.039608\n",
      "unknown        0.000167\n",
      "Name: proportion, dtype: float64\n",
      "education_level\n",
      "college    0.575450\n",
      "masters    0.182260\n",
      "unknown    0.138394\n",
      "other      0.103896\n",
      "Name: proportion, dtype: float64\n",
      "city\n",
      "san francisco    0.518176\n",
      "other            0.481824\n",
      "Name: proportion, dtype: float64\n",
      "likes_dogs\n",
      "yes    0.624493\n",
      "no     0.375507\n",
      "Name: proportion, dtype: float64\n",
      "likes_cats\n",
      "no     0.522584\n",
      "yes    0.477416\n",
      "Name: proportion, dtype: float64\n",
      "religion_importance\n",
      "unknown    0.533972\n",
      "no         0.353750\n",
      "yes        0.112278\n",
      "Name: proportion, dtype: float64\n",
      "sign_importance\n",
      "unknown          0.386595\n",
      "fun              0.322624\n",
      "not important    0.279560\n",
      "important        0.011221\n",
      "Name: proportion, dtype: float64\n",
      "english_level\n",
      "okay        0.519245\n",
      "fluently    0.469301\n",
      "poorly      0.010620\n",
      "none        0.000835\n",
      "Name: proportion, dtype: float64\n",
      "spanish_level\n",
      "none        0.727821\n",
      "okay        0.116453\n",
      "poorly      0.104747\n",
      "fluently    0.050979\n",
      "Name: proportion, dtype: float64\n",
      "french_level\n",
      "none        0.869020\n",
      "poorly      0.060430\n",
      "okay        0.052482\n",
      "fluently    0.018067\n",
      "Name: proportion, dtype: float64\n",
      "chinese_level\n",
      "none        0.938952\n",
      "okay        0.026383\n",
      "fluently    0.019169\n",
      "poorly      0.015496\n",
      "Name: proportion, dtype: float64\n",
      "german_level\n",
      "none        0.948587\n",
      "poorly      0.025047\n",
      "okay        0.017967\n",
      "fluently    0.008399\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('profiles.csv')\n",
    "\n",
    "#print(df.describe())\n",
    "print(df.info())\n",
    "#df.head()\n",
    "\n",
    "#EDA\n",
    "\n",
    "#age column\n",
    "#plt.hist(df['age'])\n",
    "#plt.show() #based on the histogram, let's consider that everything above 80 years in wrong and drop those rows\n",
    "df = df[df['age'] < 80]\n",
    "\n",
    "#body type\n",
    "# 3 groups with ~20% of the answers each. All the others have less than 10%. Will keep those 3 and aggregate the others\n",
    "\n",
    "map_body_type = {\n",
    "    'average': 'average',\n",
    "    'fit': 'fit', \n",
    "    'athletic': 'athletic',  \n",
    "    'thin': 'other', \n",
    "    'curvy': 'other',\n",
    "    'a little extra': 'other',  \n",
    "    'skinny': 'other', \n",
    "    'full figured': 'other',  \n",
    "    'overweight': 'other',  \n",
    "    'jacked': 'other', \n",
    "    'used up': 'unknown',  \n",
    "    'rather not say': 'unknown'\n",
    "}\n",
    "\n",
    "df['body_type'] = df['body_type'].map(map_body_type).fillna('unknown')\n",
    "\n",
    "#diet\n",
    "map_diet = {\n",
    "    # Group 1: Anything\n",
    "    'mostly anything': 'anything',\n",
    "    'anything': 'anything',\n",
    "    'strictly anything': 'anything',\n",
    "    \n",
    "    # Group 2: Vegetarian/Vegan\n",
    "    'vegetarian': 'vegetarian',\n",
    "    'mostly vegetarian': 'vegetarian',\n",
    "    'strictly vegetarian': 'vegetarian',\n",
    "    'vegan': 'vegetarian',\n",
    "    'mostly vegan': 'vegetarian',\n",
    "    'strictly vegan': 'vegetarian',\n",
    "    \n",
    "    # Group 3: Other (includes religious-based + remaining 'other')\n",
    "    'other': 'other',\n",
    "    'mostly other': 'other',\n",
    "    'strictly other': 'other',\n",
    "    'kosher': 'other',\n",
    "    'mostly kosher': 'other',\n",
    "    'strictly kosher': 'other',\n",
    "    'halal': 'other',\n",
    "    'mostly halal': 'other',\n",
    "    'strictly halal': 'other'\n",
    "}\n",
    "\n",
    "df['diet'] = df['diet'].map(map_diet).fillna('unknown')\n",
    "# ~40% of unknowns, ~50% of anything. Might consider dropping this one\n",
    "\n",
    "#drinks\n",
    "\n",
    "map_drinks = {\n",
    "    'socially': 'socially',\n",
    "    'rarely': 'light',\n",
    "    'not at all': 'light',\n",
    "    'often': 'heavy',\n",
    "    'very often': 'heavy',\n",
    "    'desperately': 'heavy'\n",
    "}\n",
    "df['drinks'] = df['drinks'].map(map_drinks).fillna('unknown')\n",
    "\n",
    "#socially category dominates with 70%\n",
    "\n",
    "#drugs\n",
    "map_drugs = {\n",
    "    'never': 'no',\n",
    "    'sometimes': 'yes',\n",
    "    'often': 'yes',\n",
    "    'unknown': 'unknown'\n",
    "}\n",
    "\n",
    "df['drugs'] = df['drugs'].map(map_drugs).fillna('unknown')\n",
    "\n",
    "#education\n",
    "\n",
    "#will divide into education level and is student\n",
    "\n",
    "education_level_conditions = [\n",
    "    df['education'].str.contains('college', case = False, na=False),\n",
    "    df['education'].str.contains('masters', case = False, na=False),\n",
    "    df['education'].str.contains('high school|ph.d|law school|med school', case = False, na=False) #these represent a low percentage <5%, so will group them\n",
    "]\n",
    "\n",
    "education_level_choices = ['college', 'masters', 'other']\n",
    "\n",
    "df['education_level'] = np.select(education_level_conditions, education_level_choices, default = 'unknown')\n",
    "\n",
    "#print(df['education_level'].value_counts(normalize = True))\n",
    "\n",
    "is_student_conditions = [\n",
    "    df['education'].str.contains('working on', case = False, na=False),\n",
    "    df['education'].str.contains('graduated from|dropped out', case = False, na=False),\n",
    "    df['education'].str.contains('space camp', case = False, na=False) #will consider all these unknown    \n",
    "]\n",
    "\n",
    "is_student_choices = ['yes', 'no', 'unknown']\n",
    "\n",
    "df['is_student'] = np.select(is_student_conditions, is_student_choices, default = 'unknown')\n",
    "\n",
    "df = df.drop('education', axis = 1)\n",
    "\n",
    "ethnicity_conditions = [\n",
    "    df['ethnicity'] == 'white',\n",
    "    df['ethnicity'] == 'asian',\n",
    "    df['ethnicity'].isna()\n",
    "]\n",
    "\n",
    "ethnicity_choices = ['white', 'asian', 'unknown']\n",
    "\n",
    "df['ethnicity'] = np.select(ethnicity_conditions, ethnicity_choices, default = 'other')\n",
    "\n",
    "#height\n",
    "#plt.hist(df['height'])\n",
    "#plt.show()\n",
    "df = df[(df['height'] >= 50) & (df['height'] <= 90)] #this is a reasonable interval for height\n",
    "\n",
    "#income\n",
    "# 80% of unknowns, not a great column to predict other columns, but might be fun to remove the unknowns and predict this one\n",
    "\n",
    "#plt.hist(df_income['income'])\n",
    "#plt.show()\n",
    "\n",
    "def classify_income(i):\n",
    "    if i <= 0:\n",
    "        return 'unknown'\n",
    "    elif i <= 20000:\n",
    "        return 'low'\n",
    "    elif i <= 50000:\n",
    "        return 'medium'\n",
    "    else: \n",
    "        return 'high'\n",
    "\n",
    "df['income'] = df['income'].apply(classify_income)\n",
    "\n",
    "#print(df['income'].value_counts(normalize = True))\n",
    "\n",
    "#job\n",
    "\n",
    "job_map = {\n",
    "    # STEM/Technical\n",
    "    'science / tech / engineering': 'stem',\n",
    "    'computer / hardware / software': 'stem',\n",
    "\n",
    "    # Creative/Artistic\n",
    "    'artistic / musical / writer': 'creative',\n",
    "    'entertainment / media': 'creative',\n",
    "\n",
    "    # Business/Professional\n",
    "    'sales / marketing / biz dev': 'business',\n",
    "    'executive / management': 'business',\n",
    "    'banking / financial / real estate': 'business',\n",
    "    'law / legal services': 'business',\n",
    "\n",
    "    # Education/Health\n",
    "    'education / academia': 'education_health',\n",
    "    'medicine / health': 'education_health',\n",
    "\n",
    "    # Manual Labor / Skilled Trades - Not enough percentage to create its own category\n",
    "    'construction / craftsmanship': 'other',\n",
    "    'transportation': 'other',\n",
    "    'clerical / administrative': 'other',\n",
    "\n",
    "    # Public/Government - Not enough percentage to create its own category\n",
    "    'political / government': 'other',\n",
    "    'military': 'other',\n",
    "\n",
    "    # Other groups\n",
    "    'student': 'student',\n",
    "    'unemployed': 'other',\n",
    "    'retired': 'other',\n",
    "    'rather not say': 'unknown',\n",
    "    'hospitality / travel': 'other',\n",
    "    'other': 'other'\n",
    "}\n",
    "\n",
    "df['job'] = df['job'].map(job_map).fillna('unknown')\n",
    "\n",
    "#last online\n",
    "\n",
    "df['last_online'] = pd.to_datetime(df['last_online'], format='%Y-%m-%d-%H-%M')\n",
    "most_recent_date = df['last_online'].max()\n",
    "df['last_online'] = (most_recent_date - df['last_online']).dt.days\n",
    "\n",
    "#binary distribution into active or not active. Could also make a semi active category (<=1, <=3, >3)\n",
    "def categorize_last_online(v):\n",
    "    if v <= 3:\n",
    "        return 'active'\n",
    "    else:\n",
    "        return 'not active'\n",
    "\n",
    "df['last_online'] = df['last_online'].apply(categorize_last_online)\n",
    "\n",
    "#location\n",
    "\n",
    "def get_city(location):\n",
    "    city = location.split(',')[0]\n",
    "    if city == 'san francisco': #50% of the states are san francisco, so will do this and 'other'\n",
    "        return city\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "df['city'] = df['location'].apply(get_city)\n",
    "\n",
    "#offspring\n",
    "df['offspring'] = df['offspring'].fillna('unknown')\n",
    "\n",
    "#will drop the column because there are 60% of unknowns and spliting between has and wants kids made it worse.\n",
    "\n",
    "df.drop('offspring', axis = 1)\n",
    "\n",
    "#orientation\n",
    "\n",
    "map_orientation = {\n",
    "    'straight': 'straight',\n",
    "    'gay': 'queer',\n",
    "    'bisexual': 'queer'\n",
    "}\n",
    "\n",
    "df['orientation'] = df['orientation'].map(map_orientation).fillna('unknown')\n",
    "\n",
    "#pets\n",
    "df['pets'] = df['pets'].fillna('unknown')\n",
    "\n",
    "def likes_dogs(pets):\n",
    "    if isinstance(pets, str):\n",
    "        pets = pets.lower().strip()\n",
    "        if 'likes dogs' in pets or 'has dogs' in pets:\n",
    "            return 'yes'\n",
    "    return 'no'\n",
    "\n",
    "def likes_cats(pets):\n",
    "    if isinstance(pets, str):\n",
    "        pets = pets.lower().strip()\n",
    "        if 'likes cats' in pets or 'has cats' in pets:\n",
    "            return 'yes'\n",
    "    return 'no'\n",
    "        \n",
    "\n",
    "df['likes_dogs'] = df['pets'].apply(likes_dogs)\n",
    "df['likes_cats'] = df['pets'].apply(likes_cats)\n",
    "\n",
    "#religion\n",
    "df['religion'] = df['religion'].fillna('unknown')\n",
    "\n",
    "def get_religion(r):\n",
    "    try:\n",
    "        religion = r.split(' ')[0]\n",
    "    except:\n",
    "        print(r)\n",
    "        return\n",
    "    if religion == 'buddhism' or religion == 'hinduism' or religion == 'islam':\n",
    "        return 'other'\n",
    "    else:\n",
    "        return religion\n",
    "\n",
    "religion_importance_conditions = [\n",
    "    df['religion'].str.contains('not too serious|laughing', case=False, na=False),\n",
    "    df['religion'].str.contains('somewhat serious|very serious', case=False, na=False),\n",
    "    df['religion'].str.contains('unknown', case=False, na=False)\n",
    "]\n",
    "\n",
    "religion_importance_options = ['no', 'yes', 'unknown']\n",
    "\n",
    "df['religion_importance'] = np.select(religion_importance_conditions, religion_importance_options, default = 'unknown')\n",
    "df['religion'] = df['religion'].apply(get_religion)   \n",
    "\n",
    "#sex - good to go\n",
    "#print(df['sex'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "#sign\n",
    "df['sign'] = df['sign'].fillna('unknown')\n",
    "df['sign'] = df['sign'].str.replace('&rsquo;', '\\'', regex=False)\n",
    "\n",
    "def get_sign(sign):\n",
    "    if isinstance(sign, str):\n",
    "        return sign.split(' ')[0]\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "sign_importance_conditions = [\n",
    "    df['sign'].str.contains('fun', regex = False),\n",
    "    df['sign'].str.contains('but it doesn\\'t matter', regex = False),\n",
    "    df['sign'].str.contains('and it matters a lot', regex = False)\n",
    "]\n",
    "\n",
    "sign_importance_options = ['fun', 'not important', 'important']\n",
    "\n",
    "df['sign_importance'] = np.select(sign_importance_conditions, sign_importance_options, default = 'unknown')\n",
    "df['sign'] = df['sign'].apply(get_sign)\n",
    "\n",
    "#print(df['sign'].value_counts(normalize=True, dropna=False)) # 12 categories + unknown -> not sure if it will have predictive power. Might drop it\n",
    "#print(df['sign_importance'].value_counts(normalize=True, dropna=False))  # only 1% considers important, might drop it\n",
    "\n",
    "df.drop(['sign', 'sign_importance'], axis = 1)\n",
    "\n",
    "#smokes\n",
    "df['smokes'] = df['smokes'].fillna('unknown')\n",
    "\n",
    "smokes_map = {\n",
    "    'no': 'no',\n",
    "    'unknown': 'unknown',\n",
    "    'sometimes': 'yes',\n",
    "    'when drinking': 'yes',\n",
    "    'trying to quit': 'yes',\n",
    "    'yes': 'yes'\n",
    "}\n",
    "\n",
    "df['smokes'] = df['smokes'].map(smokes_map)\n",
    "\n",
    "#print(df['smokes'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "#speaks - has a list of comma separated values with proficiency between brackets\n",
    "# will get the top 5 languages and save the proficiency level for each + add a column for languages count\n",
    "df['speaks'] = df['speaks'].fillna('unknown')\n",
    "\n",
    "def get_languages_count(l):\n",
    "    count_commas = l.count(',')\n",
    "    return (count_commas + 1)\n",
    "\n",
    "df['language_count'] = df['speaks'].apply(get_languages_count)\n",
    "#print(df['language_count'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "#TO DO -> get the 5 top languages and create columns with the level\n",
    "\n",
    "language_counts = {}\n",
    "def get_top_languages(lang):\n",
    "    languages = lang.split(',')\n",
    "    for language in languages:\n",
    "        l = language.strip().split(' ')[0]\n",
    "        if l in language_counts:\n",
    "            language_counts[l] += 1\n",
    "        else:\n",
    "            language_counts[l] = 1\n",
    "\n",
    "df['speaks'].apply(get_top_languages)\n",
    "top_5_languages = sorted(language_counts, key=language_counts.get, reverse=True)[:5]\n",
    "print(top_5_languages)\n",
    "\n",
    "for lang in top_5_languages:\n",
    "    df[f'{lang}_level'] = 'none'\n",
    "\n",
    "for idx, row in df['speaks'].items():\n",
    "    languages = row.split(',')\n",
    "    for language in languages:\n",
    "        record = language.strip().split(' ')\n",
    "        l = record[0].strip().lower()\n",
    "        if len(record) == 1:\n",
    "            level = 'okay'\n",
    "        elif len(record) >= 2:\n",
    "            level = record[1].strip('()').lower()\n",
    "        if l in top_5_languages:\n",
    "            df.at[idx, f'{l}_level'] = level        \n",
    "\n",
    "#status\n",
    "df['status'] = df['status'].fillna('unknown')\n",
    "#print(df['status'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "status_map = {\n",
    "    'single': 'available',\n",
    "    'seeing someone': 'unavailable',\n",
    "    'available': 'available',\n",
    "    'married': 'unavailable',\n",
    "    'unknown': 'unknown'\n",
    "}\n",
    "\n",
    "df['status'] = df['status'].map(status_map)\n",
    "\n",
    "#print(df['status'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "df = df.drop(['offspring', 'location', 'pets', 'speaks', 'is_student'], axis=1)\n",
    "not_essay_cols = [col for col in df.columns if 'essay' not in col]\n",
    "for col in not_essay_cols:\n",
    "    if df[col].dtype in ['object', 'category']:\n",
    "        print(df[col].value_counts(normalize=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2964e929-32b0-461b-beb8-dbdb4066f770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income\n",
      "unknown    48404\n",
      "high        5509\n",
      "medium      3026\n",
      "low         2948\n",
      "Name: count, dtype: int64\n",
      "income\n",
      "high      2948\n",
      "low       2948\n",
      "medium    2948\n",
      "Name: count, dtype: int64\n",
      "['high' 'low' 'medium']\n",
      "Selected features from Chi2: ['age', 'body_type_other', 'drugs_yes', 'job_other', 'job_stem', 'job_student', 'sign_unknown', 'smokes_yes', 'education_level_masters', 'city_san francisco']\n",
      "0    0.333428\n",
      "1    0.333286\n",
      "2    0.333286\n",
      "Name: proportion, dtype: float64\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Classification report for XGB Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.66      0.63       589\n",
      "           1       0.70      0.62      0.65       590\n",
      "           2       0.46      0.47      0.46       590\n",
      "\n",
      "    accuracy                           0.58      1769\n",
      "   macro avg       0.59      0.58      0.58      1769\n",
      "weighted avg       0.59      0.58      0.58      1769\n",
      "\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "450 fits failed out of a total of 1200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "150 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "72 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "42 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "38 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "44 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\pedro\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [0.53990388 0.57493968 0.55276505 0.5759068         nan 0.5759068\n",
      "        nan        nan 0.53990388 0.57493968 0.55276505 0.5759068\n",
      "        nan 0.57141383        nan        nan 0.53990388 0.57510286\n",
      " 0.55276505 0.5759068         nan 0.57493968        nan        nan\n",
      " 0.53990388 0.57493968 0.55276505 0.57627062        nan 0.57627062\n",
      "        nan        nan 0.53990388 0.57510286 0.55276505 0.5762387\n",
      "        nan 0.57141383        nan        nan 0.53990388 0.57510286\n",
      " 0.55276505 0.57627062        nan 0.57510286        nan        nan\n",
      " 0.5737191  0.5865967  0.5730156  0.58541497        nan 0.58541497\n",
      "        nan        nan 0.5737191  0.5865967  0.5730156  0.58541497\n",
      "        nan 0.58564879        nan        nan 0.5737191  0.5865967\n",
      " 0.5730156  0.58541497        nan 0.58629486        nan        nan\n",
      " 0.5737259  0.58633733 0.5730156  0.58544656        nan 0.58535185\n",
      "        nan        nan 0.5737259  0.58633733 0.5730156  0.58544656\n",
      "        nan 0.58563842        nan        nan 0.5737259  0.58621514\n",
      " 0.5730156  0.58544656        nan 0.58633733        nan        nan\n",
      " 0.57748442 0.58655261 0.57754363 0.58535535        nan 0.58535535\n",
      "        nan        nan 0.57748442 0.58655261 0.57754363 0.58589661\n",
      "        nan 0.58612425        nan        nan 0.57748442 0.58655261\n",
      " 0.57754363 0.58589661        nan 0.58655261        nan        nan\n",
      " 0.57758254 0.58655261 0.57754363 0.58603178        nan 0.58549053\n",
      "        nan        nan 0.57758254 0.58655261 0.57754363 0.58539436\n",
      "        nan 0.58628064        nan        nan 0.57758254 0.58655261\n",
      " 0.57754363 0.58549053        nan 0.58655261        nan        nan\n",
      " 0.57902855 0.58648839 0.57901622 0.58649707        nan 0.58648839\n",
      "        nan        nan 0.57902855 0.58662172 0.57901622 0.58649707\n",
      "        nan 0.58662172        nan        nan 0.57902855 0.58648839\n",
      " 0.57901622 0.58649707        nan 0.58648839        nan        nan\n",
      " 0.5789141  0.58664428 0.57872538 0.58648839        nan 0.58652073\n",
      "        nan        nan 0.5789141  0.58664428 0.57872538 0.58652073\n",
      "        nan 0.58664688        nan        nan 0.5789141  0.58664428\n",
      " 0.57872538 0.58648839        nan 0.58664428        nan        nan\n",
      " 0.57878228 0.58649707 0.57878228 0.58636374        nan 0.58649707\n",
      "        nan        nan 0.57878228 0.58649707 0.57878228 0.58649707\n",
      "        nan 0.58649707        nan        nan 0.57878228 0.58649707\n",
      " 0.57878228 0.58636374        nan 0.58649707        nan        nan\n",
      " 0.57878228 0.58652073 0.57878228 0.58652073        nan 0.58652073\n",
      "        nan        nan 0.57878228 0.58652073 0.57878228 0.58664688\n",
      "        nan 0.58652073        nan        nan 0.57878228 0.58652073\n",
      " 0.57878228 0.58652073        nan 0.58652073        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for LR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.65      0.63       589\n",
      "           1       0.66      0.68      0.67       590\n",
      "           2       0.47      0.42      0.44       590\n",
      "\n",
      "    accuracy                           0.58      1769\n",
      "   macro avg       0.58      0.58      0.58      1769\n",
      "weighted avg       0.58      0.58      0.58      1769\n",
      "\n",
      "Best Parameters: {'C': 10, 'class_weight': 'balanced', 'l1_ratio': 0.5, 'penalty': 'elasticnet', 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, SelectKBest, chi2\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def downsample(df, col):\n",
    "    min_count = df[col].value_counts().min()\n",
    "    balanced_df = pd.concat([\n",
    "        df[df[col] == category].sample(n=min_count, random_state=1)\n",
    "        for category in df[col].unique()\n",
    "    ])\n",
    "    return balanced_df\n",
    "\n",
    "data = df[not_essay_cols].copy()\n",
    "col_to_predict = 'income'\n",
    "print(data[col_to_predict].value_counts())\n",
    "\n",
    "#remove rows where the target variable is unknown\n",
    "data = data[data[col_to_predict] != 'unknown']\n",
    "data = data[data[col_to_predict].notna()]\n",
    "\n",
    "data = downsample(data, col_to_predict)\n",
    "\n",
    "#create vars with categorical columns and numerical columns\n",
    "cat_cols = data.select_dtypes(include=['object','category']).columns.tolist()\n",
    "cat_cols = [col for col in cat_cols if col != col_to_predict]\n",
    "num_cols = data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "num_cols = [col for col in num_cols if col != col_to_predict]\n",
    "\n",
    "df_dummies = pd.get_dummies(data[cat_cols], drop_first = True)\n",
    "\n",
    "x = pd.concat([data[num_cols], df_dummies], axis = 1)\n",
    "y = data[col_to_predict]\n",
    "\n",
    "print(data[col_to_predict].value_counts()) #check if the classes are balanced\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(le.classes_)\n",
    "\n",
    "# ------------ Chi2 test ------------ #\n",
    "# Chi2 requires all values to be non-negative\n",
    "X_chi = x.copy()\n",
    "X_chi[X_chi < 0] = 0  # Only necessary if you have negative values\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=10)  # Keep top 10 features\n",
    "X_chi_selected = selector.fit_transform(X_chi, y)\n",
    "selected_features = X_chi.columns[selector.get_support()]\n",
    "print(f\"Selected features from Chi2: {list(selected_features)}\")\n",
    "\n",
    "x = x[selected_features]\n",
    "\n",
    "cat_cols = x.select_dtypes(include=['object','category']).columns.tolist()\n",
    "num_cols = x.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# ------------ End of Chi2 test ------------ #\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=24, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "if len(num_cols) > 0:\n",
    "    x_train[num_cols] = scaler.fit_transform(x_train[num_cols])\n",
    "    x_test[num_cols] = scaler.transform(x_test[num_cols]) #never use fit transform on test data, because it will learn from train data\n",
    "\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "\n",
    "# ----- XGB Classifier ----- #\n",
    "\n",
    "model = XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1_macro', verbose=1, n_jobs=-1)    \n",
    "\n",
    "gs.fit(x_train, y_train)\n",
    "y_pred = gs.predict(x_test)\n",
    "print('Classification report for XGB Classifier:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Best Parameters:\", gs.best_params_)\n",
    "\n",
    "# --------- Logistic Regression ------------- #\n",
    "\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
    "    'solver': ['liblinear', 'saga'],  # Solvers that support l1 and elasticnet\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'l1_ratio': [0, 0.5, 1]  # Only used with elasticnet\n",
    "}\n",
    "gs = GridSearchCV(model, param_grid, cv=5, scoring='f1_weighted', verbose=1, n_jobs=-1)\n",
    "\n",
    "gs.fit(x_train, y_train)\n",
    "y_pred_model = gs.predict(x_test)\n",
    "model_score = classification_report(y_test, y_pred_model)\n",
    "print('Classification report for LR:')\n",
    "print(model_score)\n",
    "print(\"Best Parameters:\", gs.best_params_)\n",
    "\n",
    "#TRY K NEAREST NEIGHBORS AND DECISION TREE\n",
    "\n",
    "#USE NAIVE BAYS CLASSIFIER TO PREDICT SOMETHING BASED ON THE ESSAYS\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
